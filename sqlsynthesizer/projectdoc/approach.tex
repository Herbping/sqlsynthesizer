
\section{Approach}
\label{sec:approach}

Before presenting our SQL query inference algorithm, we first describe
the supported SQL language subset.

\newcommand{\q}{\langle query\rangle}
\newcommand{\db}{\langle db\rangle}
\newcommand{\pat}{\langle pat\rangle}
\newcommand{\bug}{\langle bug\rangle}
\newcommand{\dist}{\langle distance\rangle}
\newcommand{\sem}[1]{\llbracket #1\rrbracket}
\newcommand{\lit}[1]{\texttt{#1}}

\newcommand{\column}{\langle column\rangle}
\newcommand{\dbtable}{\langle table\rangle}
\newcommand{\cond}{\langle cond\rangle}
\newcommand{\op}{\langle op\rangle}
\newcommand{\e}{\langle expr\rangle}
\newcommand{\ce}{\langle cexpr\rangle}

\begin{figure}[t]
%\scriptsize{%
\footnotesize%
\begin{align*}
\q ::= {} 
	& \texttt{ SELECT } \e^+ \texttt{ FROM } \dbtable^+ \\
        & \texttt{ WHERE } \cond^+ \\ 
	&  \texttt{ GROUP BY } \column^+ \texttt{ HAVING } \cond^+\\
\dbtable::= {} &\ atom \\
\column ::= {} &\ \dbtable.atom\\
\cond ::= {} &\ \ \cond \;\texttt{\&\&}\; \cond \\ 
    & |\ \cond \;\texttt{||}\; \cond \\
    & |\ \texttt{(}\;\cond\;\texttt{)} \\
    & |\ \ce \;\op\; \ce \\
\op ::= {} &\ \ \texttt{=} \;\;|\;\; \texttt{>}  \;\;|\;\; \texttt{<}\\
\ce ::= {} &\ \ const \;\;|\;\; \column  \;\; \\
\e ::= {} & \ce \\
    & |\ sum(\column) \;\;|\ max(\column) \;\;|\ min(\column) 
\end{align*}
\normalsize%
\caption{Syntax of the supported SQL language subset.}
\label{fig:syntax}
\end{figure}


\subsection{Language}

Figure~\ref{fig:syntax} defines the syntax of the supported, which
is a subset of the standard query language SQL. This language subset
supports common query operations using multiple tables (i.e.,
\CodeIn{select} ... \CodeIn{from}... \CodeIn{where}..), and standard
boolean operators as predicates. The language
subset shares the same semantics with the standard SQL language.
Different than existing work~\cite{DasSarma:2010}, this language supports
joining operations across multiple tables.  In addition, the language includes
widely used database operations such as \CodeIn{group by},
\CodeIn{order by}, and \CodeIn{having}, as
well as a few common aggregation functions such as \CodeIn{sum},
\CodeIn{max}, and \CodeIn{min}.

Compared to the full SQL specification, this language subset is far from complete.
However, there is a tradeoff between the completeness (or expressiveness) of a language,
and the complexity of finding a simple consistent program (here, a SQL query)
within that space defined by the language. In general, the more expressive
a language, the harder the task of effectively finding an answer. When
designing the language subset, we balanced three inter-related principles:

\begin{itemize}

\item \textit{Expressiveness}. The language subset be able
to represent a reasonable set of SQL queries in practice.

\item \textit{Generality}. The language should resemble
the existing SQL language and should not impose additional
or different semantics.

\item \textit{Tool utility}. Inferring a consistent query
using this language subset should be easy to leverage by
domain knowledges and other analyses.

\end{itemize}

As these principles stand in tension, our supported language subset
represents a sweet spot that addressed each of them to a reasonable
extent. As we will show in Section~\ref{sec:evaluation}, this
language subset is expressive enough for most SQL query exercise
after a widely-used textbook and many online forum answers.


\subsection{Inference Algorithm}

We describe an algorithm to infer good SQL queries from the given
input-output examples. Our algorithm consists of three steps:
table alignment (Section~\ref{sec:alignment}),
query completion (Section~\ref{sec:completion}), and
candidate ranking (Section~\ref{sec:ranking}).

\yuyin{can you please wrote the following 3 subsections? I wrote
down a few of my thoughts, which are only for reference. Feel free
to ignore them, and express your own ideas. }

\subsubsection{Table Alignment}
\label{sec:alignment}

Given an input-output example, this step infers a rough form of the SQL query that should be returned. We want to infer which tables are used in the SQL query and by which column they are joined together.

The first observation is that if input example contains multiple tables, it is quite likely that the inferred SQL query will use all tables for at least one time. So the lower bound of table number should be the number of all tables ($n_t$) provided by input example. It's possible that each table is used for more than one time. However this actually generate a more complex hypothesis with poor generalization ability. Based on Occam's razor, we want to use as few tables as possible in our inferred SQL query. At very beginning our basic table set  $T$ contains all input tables $t_1,..., t_{n_t}$. If one field of the input table appears more than once in output table, this strongly indicates the same table will be joined many times in the inferred SQL query. At this time, we will add this table to our table set $T$ using an alias.

The second observation is that only fields whose data types are the same and meanings are close to each other can be joined. To judge if two fields from different tables have the similar meaning, we could check the intersection of their content. Two fields which have a large portion of overlap may belong to the similar concept.

The third observation is that if there is a column in output table whose name doesn't come from our original columns' name set, it may be the result of aggregation.

The forth observation is that group by always come with aggregation. If there are at least two columns in output table and there is aggregation operation, we will also add group by operation. The column by which we group can be inferred from output table.

All possible tables generated by joining tables from basic table set $T$ on fields which are joinable compose our primary search space.
%Although each table may appear more than one time.
%Yet each table may appear more than one time. In order to make sure our searching space covers the right SQL query with high possibility we assume that each table may appear up to 2 times in the FROM clause. So the upper bound of table number should be 3 times the number of all input tables.


The bound of table number is $O(n_t!)$, the bound of possible number of join is $O(c_t^2)$ and the bound of the number of conditions is $O(n_t!n_tc_t^2)<O(n_t^3c_t^2)$.

%For example, from input-output examples, we can roughly know which
%possible constructions (select, join, group by, or having) will
%be used. This step can reduce the overall search space.

\subsubsection{Query Completion}
\label{sec:completion}

The SQL skeleton, though incomplete, serves as a good reference in
inferring complete and valid SQL queries.

After getting a rough idea of what the resulting SQL query would
look like, this step completes the remaining uncertain parts by
searching. Smarter search would be preferable.

In this part, we focus on completing conditions. We tried to figure out the right query condition using a decision tree based method. After each cut, we want to make sure all output examples can be kept. To select which column is used, we could compute the uncertainty on the rest of columns. The column which reduces the uncertainty the most will be chosen.

After all possible conditions are found out, we will remove some tables which haven't used these conditions at all.

%How to make a reasonable guess of the remaining parts.

\subsubsection{Candidate Ranking}
\label{sec:ranking}

After the first two step, several SQL queries which satisfy the given input-output examples will be returned. Due to lacking in input-output examples we are not be able to rule out SQL queries that lack in generalization ability. Instead we will provide user queries according to a rank.

Here we use a simple but useful strategy based on Occam's razor to rank the inferred SQL queries. For all queries, each table which they use will cost a cost $C_t$ and each predicate which they use will result in a cost $C_p$. The total number of cost of a query is $n_t*C_t+n_p*C_p$ in which $n_t$ is the number of tables and $n_p$ is the number of predicates used in the query. The lower the cost the higher the rank.

%If a list of multiple SQL queries satisfy the given input-output examples, how
%to present the most likely queries to the top?

