
\section{Evaluation}
\label{sec:evaluation}

We implemented our approach as an open-source programming tool. Our tool, implemented
in Java, uses MySQL as the backend database engine. When a SQL query is synthesized,
our tool executes this query on the backend database, and checks whether the output
query result matches the given example output.

We next describe the evaluation of our prototype tool.

\subsection{Research Questions}

We aim to investigate the following research questions:

\begin{itemize}
\item Is the supported SQL subset expressive enough to describe
queries that real end-users require?

\item Can our inference algorithm efficiently find a SQL query
from input-output examples?

\item Does the inferred SQL query correctly express end-users'
intention and produce the expected output? If not, how many
additional examples does a user need to provide before our
approach infers a correctly-behaved SQL query?

\end{itemize}


\subsection{SQL Query Synthesis Scenarios}

To evaluate our technique, we collected two types of
SQL query synthesis scenarios. First, we picked
up 6 SQL exercises from a classic database textbook~\cite{cowbook}.
Those 6 SQL exercises cover a the most commonly-used SQL features,
and can be used as golden test to check the expressiveness
of our supported SQL subset. Second, we collected
5 SQL problems rasied by real end-users from popular online help forums.

%evaluation
%subjects from two major sources: exercises from classic
%database textbooks like, and real-world questions
%that end-user asked on well-known online help forums. Specifically,
%we aim to apply our implemented tool solve SQL query related questions
%after Chapter 3 (SQL DML) in~\cite{cowbook}. We have also collected
%19 end-user questions from online forums, and plan to apply our
%tool to solve them.

For a given exercise or problem, if it
has already been associated with input-output examples (e.g.,
those from online forums), we directly apply our tool on the examples.
Otherwise, we manually provide a few concise and representative
examples for our tool.
If for a given exercise or problem, our tool inferred
a SQL query that does not behave as we expected when applied
to other inputs, we will manually find an input on which the
SQL query misbehaves and reapplied our tool to the new input. We
repeat this process until our tool infers a desirable SQL query.

\subsection{Experimental Results}


\subsubsection{A Real Example}
\input{realproblem}

\subsection{Experiemtnal Conclusions}
