
\section{Evaluation}
\label{sec:evaluation}

We evaluated four aspects of \ourtool's effectiveness,
answering the following research questions:

\begin{itemize}
\item What is the success ratio of \ourtool in synthesizing
SQL queries? (Section~\ref{sec:ratio}).
%Is the supported SQL subset expressive enough to describe a variety of queries?
\item How long does it take for \ourtool to
synthesize a SQL query (Section~\ref{sec:performance}).
\item How much human effort is needed to write sufficient
input-output examples for SQL synthesis (Section~\ref{sec:human}).
\item How does \ourtool's effectiveness compare to
existing SQL query inference techniques (Section~\ref{sec:comparison}).
\end{itemize}






\subsection{Benchmarks}

%We collected benchmarks from two sources, and show them 
%in Figure~\ref{tab:results}.

Our benchmarks are shown in Figure~\ref{tab:results}.


\begin{itemize}
\item We used \allex SQL query related exercises
from a classic database textbook~\cite{cowbook}.
These exercises are from Chapter 5, which systematically
introduces the SQL language. We choose textbook exercises because they
are designed to
cover a wide range of SQL features. Some exercises
are even designed on purpose to cover some less realistic,
corner cases in using SQL. Like the motivating
example in Section~\ref{sec:example}, all exercises are about
writing a SQL query that retrieve data from a database.
We used \textit{all} exercises that can be expressed in the standard
SQL language without requiring vendor-specific
features or user-defined numeric functions.
As shown in Figure~\ref{tab:results},
most textbook exercises involve at least 3 tables. It was unintuitive
for us to write the correct query by simply looking at the problem
description.

\item We searched SQL query related questions raised by real-world
database users from 3 popular online forums~\cite{stackoverflow,
tutorialized, dbjournal}.
We focused on questions about how to use standard SQL features.
We excluded questions that were vaguely described or obviously
wrong, and discarded questions that had been proved
to be unsolvable by using SQL (e.g., computing a
transitive closure).
We collected \pnum non-trivial forum questions related to writing a SQL query.
One question even did not receive any reply.
Writing a good forum post is often harder than asking
around a SQL expert (since the post has to clearly 
describe the problem), and these end-users had already tried but
failed to find the correct SQL query before they wrote the post.
\end{itemize}



\subsection{Evaluation Procedure}

We used \ourtool to solve each textbook exercise and forum
question. If an exercise or problem
was associated with example input and output,
we directly applied \ourtool on those examples.
Otherwise, we manually wrote some example input and output.
To reduce the bias in writing
examples, all examples are written by a graduate
student (whose research field is not database-related) from University of Washington rather than
\ourtool's developers.

We checked \ourtool's correctness by comparing its
output with the expected SQL queries.
For textbook exercises, we compared \ourtool's output with
their correct answers; for forum questions, we
manually wrote the correct SQL query and then
compared it with \ourtool's output.
%determined
%whether \ourtool can produce it.
%the output query can fulfill
%the query task or not.

For some textbook exercises and forum questions,
\ourtool inferred a SQL query that satisfied the input-output
examples, but did not behave as the user expected.
We manually found an input on which the
SQL query mis-behaved and re-applied \ourtool to the new input. We
repeated this process and recorded the number of
interactions before \ourtool output a correct SQL query.


All experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory, running Windows 7.



\input{results}


\subsection{Results}

Figure~\ref{tab:results} summarizes our experimental results.

\subsubsection{Success Ratio}
\label{sec:ratio}


As shown in Figure~\ref{tab:results}, \ourtool
synthesized correct SQL queries for \solexnum  out of
\exnum the textbook exercises, and \solpnum out of
\pnum the forum questions.
\ourtool failed to solve XXX textbook exercises, for
two reasons. XXX exercises 
%per exercise (or problem).
\todo{why the technique can work, why some problem
can not be solved}

We also observed that our ranking strategy (Section~\ref{sec:ranking})
was quite effective: for most benchmarks, it ranked the correct
SQL query as one of top XXX suggestions.

%\input{realproblem}

\subsubsection{Performance}
\label{sec:performance}

We measured \ourtool's performance by recording the
time cost in producing a ranked list of SQL queries.
As shown in Figure~\ref{tab:results}, on average,
including benchmarks that \ourtool failed to produce
a correct answer,
\ourtool took less than \avgtime minutes in total to
produce the results (max: xx, min: xx).
For benchmarks on which \ourtool succeeded, the average
time cost was xxx minutes (max: xxx, min: xxx).
Most of the time is spent querying the backend
database to validate the correctness of each synthesized  SQL query.
\ourtool's speed makes it an attractive tool to replace
the role of the SQL experts, which
enables end-users to solve their problems in a few
minutes.

%\todo{caching might be helpful}



\subsubsection{Human Efforts}
\label{sec:human}

We measured the human efforts taken to use \ourtool in two ways.
First, the time cost to write sufficient input-output examples. Second,
the number of interactive rounds in invoking \ourtool
to synthesize the correct SQL queries.

As shown in Figure~\ref{tab:results}, human efforts
spent in providing input-output examples are very limited:
on average, it took less than \avghum minutes for one benchmark
(max: XXX, min: xxx).
On average, an example input and output pair containing \avgtuple
rows in total is sufficient (max: XXX, min: XX).
\ourtool typically requires just \avground rounds of
interaction (max: XXX, min: XXXX). For most benchmarks, the number of interaction
rounds ranges from XXX to XXX.
The maximum number
of interactive rounds required in our benchmarks\
was XXX. \todo{the largest table}
%The maximum number of examples
%required in any scenario over all possible interactions was 10.


\subsubsection{Comparison with an Existing Technique}.
\label{sec:comparison}

We compared \ourtool with \textit{Query By Output} (QBO), an
approach to infer SQL queries~\cite{Tran:2009} from examples.
We chose QBO because it is the most recent technique and also one
of the most accurate SQL query inference techniques in
the literature. QBO requires an example input-output pair, and
uses the decision tree algorithm to infer a query.
However, QBO has three fundamental limitations. First, 
it can only join two tables on their key columns (annotated by users), and requires
users to specify how to project the results
by annotating the projection columns.
Second, it only uses the original tuple values
in input tables as learning features, and thus can only
infer simple query conditions. Third, QBO does not support
many useful SQL features, such as aggregates, the \CodeIn{GROUP BY}
clause, and the \CodeIn{HAVING} clause.

We implemented QBO as a special case in \ourtool, annotated
each example table as required by QBO, and run it
on our benchmarks. Its results are shown in Figure~\ref{tab:results}.
For all \exnum database exercises
and \pnum forum questions, QBO only produces correct answers
for XXX and XXX of them, respectively.
Without surprise, \textit{all}
benchmarks solved by QBO can also be solved by \ourtool.
QBO failed to synthesize correct SQL queries for other benchmarks, due
to its limited support for learning join conditions,
projection columns, query conditions, as well as many other SQL features.

We did not compared \ourtool with other related techniques~\cite{Howe:2011,
abs-1208-2013, Harris:2011, Kandel:2011}, for two reasons. First,
some techniques such as~\cite{Khoussainova:2010, Howe:2011, abs-1208-2013},
require completely different input (e.g., a query log~\cite{Khoussainova:2010, Howe:2011}
or a snippet of Java code~\cite{abs-1208-2013}) than \ourtool.
Second, other techniques produce completely different
output (e.g., an excel transformation macro~\cite{Harris:2011}, or a
text editing script~\cite{Kandel:2011}) than \ourtool. All these
factors make it hard to conduct a meaningful comparison.

\subsection{Experimental Discussion}

\noindent \textbf{\textit{Limitations.}}
The experiments indicate three limitations
of our technique. First, some query tasks
cannot be formulated by our SQL subset (Section~\ref{sec:langsubset})
due to unsupported features, such as
nested queries. This limitation is expected;
and our future work
should address this by including more SQL
features in \ourtool.
Second, on some examples, the learned query
conditions, though correct, are not precise
enough; and require users to provide more informative examples.
Take the example input and output in Figure~\ref{fig:rank}
as an example, \ourtool produces a SQL
query \CodeIn{select name from student where score > 2}
to satisfy the examples. However, if
the condition of the expected query
is \CodeIn{score > 3}, users must provide
one more tuple to the input table, such as "Chris, 3"
(a tuple with ``Chris'' in the name column and ``3''
in the score column), while keeping the output table
unchanged, 
to guide \ourtool to learn the correct query condition.
%Such imprecision is not caused by the limitation of
%\ourtool; rather, it reflects.
Third, \ourtool requires
users to provide noise-free input-output examples.
Even in the presence of a small amount of user-input
noises (e.g., a typo), \ourtool will declare failure
when it fails to infer a valid SQL query.
To overcome this limitation, we plan to design a more
robust inference algorithm that can attempt to identify
and tolerate user-input noises, and even suggest a fix
to the noisy example.

\vspace{1mm}
\noindent \textbf{\textit{Threats to Validity.}}
There are three major threats to validity
in our evaluation. First, the \exnum textbook exercises
and \pnum forum questions, though covering
a wide variety of SQL features, may not be representative enough.
Thus, we can not claim the results can be generalized to an
arbitrary use-case scenario. Second, we only compared
\ourtool with the \textit{Query by Output} technique~\cite{Tran:2009}.
Using other query inference or recommendation techniques
might achieve different results. Third, our
experiments focus on evaluating \ourtool's generality 
and accuracy. Even though all experiments are carried
out by a different person rather than \ourtool's developers,
it is unknown about \ourtool's general usability in practice.
To address this issue, we plan to conduct
a user study in our future work.


\vspace{1mm}
\noindent \textbf{\textit{Experimental Conclusions.}}
We have three chief findings: \textbf{(1)}
The supported SQL subset in \ourtool is
able to describe a variety of database queries.
\textbf{(2)} \ourtool can efficiently synthesize desirable SQL
queries with a small amount of human
efforts and small input-output examples.
\textbf{(3)} \ourtool produces significantly better results
than an existing technique (\textit{Query by Output}~\cite{Tran:2009}).




