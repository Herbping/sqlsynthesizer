
\section{Evaluation}
\label{sec:evaluation}

We evaluate \ourtool's effectiveness in sythesizing
SQL queries. \ourtool's effectiveness can be reflected
by:

\begin{itemize}
\item the success ratio of sythesizing a variety of
SQL queries (Section~\ref{sec:ratio}).
%Is the supported SQL subset expressive enough to describe a variety of queries?
\item the time cost of sythesizing SQL queries
(Section~\ref{sec:performance}).
\item the human efforts to write input-output
examples (Section~\ref{sec:human}).
\item comparison with a previous SQL query inference
technique (Section~\ref{sec:comparison}).
\end{itemize}






\subsection{Benchmarks}

We collected benchmarks from two sources:

\begin{itemize}
\item We picked up \textit{all} SQL exericses (\allex in total) from a
classic database textbook~\cite{cowbook}. Exercises
from a textbook are good resource to evaluate \ourtool's
generality, since such exericses are often designed to
cover a wide range of SQL features. Some exericses
are even designed on purpose to cover less realistic,
corner cases in using SQL. When writing \todo{exclude
sql update, delte}

\item We searched SQL query-related questions raised by real-world
database users from 3 popular online forums~\cite{stackoverflow, tutorialized, dbjournal}.
\todo{focus on standard features, some of features, such
questions reflect how ppl use sql in practice}
As of April 2013, we collected \pnum forum questions related
to writing a SQL query.
\end{itemize}



\subsection{Evaluation Procedure}

For each textbook exericse and forum question, we use
\ourtool to solve it. If an exericse or problem has
been associated with input-output examples,
we directly apply \ourtool on those existing examples.
Otherwise, we manually write some examples based
on our understanding. To reduce the bias in writing
examples, all examples are writting by a graduate
student from University of Washington other than
\ourtool's developers.

We checked \ourtool's correctness by comparing its
output with the desired SQL queries. \todo{
    some exericses have answers, but some are not.
    If in different forms, we checked it semantic
    equivalence.}

For some textbook exercises and forum questions,
\ourtool inferred a SQL query that satisfied the input-output
examples, but did not behave as we expected when applied
to other inputs. We manually found another input on which the
SQL query mis-behaved and reapplied \ourtool to the new input. We
repeated this process until our tool inferred a desirable SQL query.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\begin{figure*}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|c|c|c||c|c|c|c|c||l|}
\hline
\multicolumn{3}{|c||}{Benchmarks} & \multicolumn{5}{|c||}{\ourtool} & Query by\\
\cline{1-8}
 ID & Source & \#Input Tables & Example Size & Rank & Tool Cost (s) & Cost in Writing Examples (s) & \#Iterations & Output~\cite{Tran:2009}\\
 \hline
 \hline
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
\hline
\end{tabular}
\Caption{{\label{tab:results} Experimental results in synthesizing SQL queries.
Column ``Benchmarks'' describes the characteristics of our benchmarks. Sub-column ``\#Input Tables''
shows the number of input tables in each benchmark. Column ``\ourtool'' shows
\ourtool's results in sythesizing SQL queries. Sub-column ``Example Size''
shows the number of rows in all example input and output tables.
Sub-column ``Rank'' shows the absolute
rank of the desirable SQL query in \ourtool's output. Sub-column ``Tool Time Cost (s)''
shows \todo{}. Sub-column ``\#Iterations'' shows the number of
interactive rounds in using \ourtool to obtain the desirable SQL query.
Column ``Query by Output'' shows the results of using a previous technique, called
\textit{Query by Output} (QBO)~\cite{}. Since \todo{treated as a special case},
we omit other. ``Y''  means QBO produces the desirable SQL queries, while ``N''
means QBO fails to produce the desirable SQL queries.
}}
\end{figure*}

\subsection{Results}

Figure~\ref{tab:results} summarizes our experimental results.

\subsubsection{Success Ratio}
\label{sec:ratio}


As shown in Figure~\ref{tab:results}, \ourtool
synthesized expected SQL queries for \solexnum  out of
\exnum the textbook exercises, and \solpnum out of
\pnum the forum questions.
%per exercise (or problem).

\todo{why the technique can work, why some problem
can not be solved}

\input{realproblem}

\subsubsection{Performance}
\label{sec:performance}

We measured \ourtool’s performance by recording the average
time cost in producing a ranked list of SQL queries. 
As shown in Figure~\ref{tab:results}, the performance of \ourtool is
reasonable. On average, it uses less than \avgtime minutes to
produce the results in one interative round.
Most of the time is spent querying the backend
database to validate the correctness of each sythesized SQL query.
%\todo{caching might be helpful}



\subsubsection{Human Efforts}
\label{sec:human}

We measured the human efforts taken to use \ourtool in two ways.
First, the time cost to write input-output examples. Second,
the number of interactive rounds in invoking \ourtool
to sythesize the desirable SQL queries.

As shown in Figure~\ref{tab:results}, human efforts
spent in providing input-output examples are very limited:
on average, it took less than 5 minutes for one benchmark.
\todo{explain some abnormal points}

The number of interactive rounds is a measure of
the generalization power of the conditional learning part
of the algorithm and the ranking scheme.
We observed that the tool typically requires
just \avground rounds of interaction, when the user is smart
enough to give an example for each input format (which
typically range from 1 to 3) to start with. This was indeed the case
for most cases in our benchmarks, even though our algorithm
can function robustly without this assumption. The maximum number
of interactive rounds required in any scenario was \todo{XXX}
(with 2 to 3 being a more typical number). \todo{the largest table}
The maximum number of examples
required in any scenario over all possible interactions was 10.

\subsubsection{Comparison with an Existing Technique}.
\label{sec:comparison}

We compared \ourtool with \textit{Query By Output} (QBO), a
data-driven approach to infer SQL queries~\cite{Tran:2009}. We chose
QBO because it is the most recent technique and also one
of the most precise SQL query inference techniques in
the literature. QBO uses a decision-tree-based algorithm
\todo{explaining what is QBO}
However, QBO cannot infer SQL queries using \todo{aggregates}

The experimental results of QBO is shown in Figure~\ref{tab:results}
(Column ``Query by Example''). For all \exnum database exercises
and \pnum forum questions, QBO produces correct answers
for XXX and XXX of them, respectively. QBO fails to
sythesize desirable SQL queries for other benchmarks, because
it \todo{the reasons}.

\subsection{Initial Experience}

We deployed the beta-test version of \ourtool
to a small number of developers and have been
using it ourselves, and refining it, since
early May 2012. Designing and deploying \ourtool, along with
feedback from the handful of users, has helped
us to better understand the issues and to improve
the tool's design.

Here is one example piece of feedback from an
external user, via private communication:

\todo{some quote from users}

Prior to developing \ourtool, we studied
over 100 forum posts about common problems
about writing correct SQL queries.
\todo{most of them use examples}
We anticipate that future user studies will
identify additional strengths and weaknesses that will allow us to
further improve \ourtool.




\subsection{Experimental Discussion}

\noindent \textbf{\textit{Limitations.}}
The experiments indicate several limitations
of our technique. First, XXX Second, XXX..
\todo{fill above}

\vspace{1mm}
\noindent \textbf{\textit{Threats to Validity.}}
There are three major threats to validity
in our evaluation. First, the \exnum textbook exercises
and \pnum forum problems may not be representative. Thus,
we can not claim the results can be generalized to an
arbitrary scenario. Second, we only compared
\ourtool with \todo{comparison results}. Using
other query inference and recommendation techniques~\cite{}
might achieve different results. Third, \todo{about user study}


\vspace{1mm}
\noindent \textbf{\textit{Experimental Conclusions.}}
We have three chief findings: \textbf{(1)}
The supported SQL subset in \ourtool is
expressive enough to describe a variety of database queries.
\textbf{(2)} \ourtool can efficiently synthesize desirable SQL
queries on \todo{how many} for both textbook exercises
and online forum problems, with a small amount of human
effort small input-output examples.
\textbf{(3)} \ourtool produces better results
than existing techniques~\cite{}. \todo{edit}




