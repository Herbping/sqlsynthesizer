
\section{Evaluation}
\label{sec:evaluation}

We evaluated four aspects of \ourtool's effectiveness,
answering the following research questions:

\begin{itemize}
\item What is the success ratio of \ourtool in sythesizing
a variety of SQL queries? (Section~\ref{sec:ratio}).
%Is the supported SQL subset expressive enough to describe a variety of queries?
\item How long does it take for \ourtool to
sythesize a SQL query (Section~\ref{sec:performance}).
\item How much human effort is needed to write sufficient
input-output examples for \ourtool in SQL sythesis (Section~\ref{sec:human}).
\item How does \ourtool's effectiveness compare to
an existing SQL query inference technique (Section~\ref{sec:comparison}).
\end{itemize}






\subsection{Benchmarks}

We collected benchmarks from two sources:

\begin{itemize}
\item We selected \textit{all} SQL query related exericses
(\allex in total) from a classic database textbook~\cite{cowbook}.
All exercises are from Chapter 5, which systematically
introduces the SQL language.
Exercises from a textbook are good resource to evaluate \ourtool's
generality, since such exericses are often designed to
cover a wide range of SQL features. Some exericses
are even designed on purpose to cover some less realistic,
corner cases in using SQL. As shown in Figure~\ref{tab:results},
each textbook exercises involves at least 3 tables. It was unintuitive
for us to write the correct query by simply looking at the problem
description in the exercise.

\item We searched SQL query related questions raised by real-world
database users from 3 popular online forums~\cite{stackoverflow, tutorialized, dbjournal}.
We focused on questions related to standard SQL features
rather than vendor-specific SQL features. We
excluded questions that were vaguely described or obviously
wrong, and discarded questions that had been proved
to be unsolvable by using SQL (e.g., computing a
transitive closure).
We collected \pnum forum questions related to writing a SQL query.
\todo{merge same types. exclude jdbc, why relative few}
\end{itemize}



\subsection{Evaluation Procedure}

We used \ourtool to solve each textbook exercise and forum
question. If an exericse or problem had
been associated with example input and output,
we directly applied \ourtool on the existing example.
Otherwise, we manually wrote some examples.
To reduce the bias in writing
examples, all examples are writting by a different graduate
student from University of Washington other than
\ourtool's developers.

We checked \ourtool's correctness by comparing its
output with the desired SQL queries. Specifically,
for textbook exericses, we compared \ourtool's output with
their correct answers, and for forum questions, we
manually determined whether the output query can fulfill
the query task or not.

For some textbook exercises and forum questions,
\ourtool inferred a SQL query that satisfied the input-output
examples, but did not behave as we expected when applied
to other inputs. We manually found another input on which the
SQL query mis-behaved and re-applied \ourtool to the new input. We
repeated this process and recorded the number of
interactions until \ourtool sythesized a desirable SQL query.


Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.


\begin{figure*}[t]
\setlength{\tabcolsep}{.24\tabcolsep}
\begin{tabular}{|c|c|c||c|c|c|c|c||l|}
\hline
\multicolumn{3}{|c||}{Benchmarks} & \multicolumn{5}{|c||}{\ourtool} & Query by\\
\cline{1-8}
 ID & Source & \#Input Tables & Example Size & Rank & Tool Cost (s) & Cost in Writing Examples (s) & \#Iterations & Output~\cite{Tran:2009}\\
 \hline
 \hline
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
 1 & Textbook Ex 5.1.1 & &  & & & & & \\
\hline
\end{tabular}
\Caption{{\label{tab:results} Experimental results in synthesizing SQL queries.
Column ``Benchmarks'' describes the characteristics of our benchmarks. Sub-column ``\#Input Tables''
shows the number of input tables in each benchmark. Column ``\ourtool'' shows
\ourtool's results in sythesizing SQL queries. Sub-column ``Example Size''
shows the number of rows in all example input and output tables.
Sub-column ``Rank'' shows the absolute
rank of the desirable SQL query in \ourtool's output. Sub-column ``Tool Time Cost (s)''
shows \todo{}. Sub-column ``\#Iterations'' shows the number of
interactive rounds in using \ourtool to obtain the desirable SQL query.
Column ``Query by Output'' shows the results of using a previous technique, called
\textit{Query by Output} (QBO)~\cite{}. Since \todo{treated as a special case},
we omit other. ``Y''  means QBO produces the desirable SQL queries, while ``N''
means QBO fails to produce the desirable SQL queries.
}}
\end{figure*}

\subsection{Results}

Figure~\ref{tab:results} summarizes our experimental results.

\subsubsection{Success Ratio}
\label{sec:ratio}


As shown in Figure~\ref{tab:results}, \ourtool
synthesized expected SQL queries for \solexnum  out of
\exnum the textbook exercises, and \solpnum out of
\pnum the forum questions.
%per exercise (or problem).

\todo{why the technique can work, why some problem
can not be solved}

%\input{realproblem}

\subsubsection{Performance}
\label{sec:performance}

We measured \ourtool’s performance by recording the average
time cost in producing a ranked list of SQL queries. 
As shown in Figure~\ref{tab:results}, the performance of \ourtool is
reasonable. On average, it uses less than \avgtime minutes to
produce the results in one interative round.
Most of the time is spent querying the backend
database to validate the correctness of each sythesized SQL query.
%\todo{caching might be helpful}



\subsubsection{Human Efforts}
\label{sec:human}

We measured the human efforts taken to use \ourtool in two ways.
First, the time cost to write input-output examples. Second,
the number of interactive rounds in invoking \ourtool
to sythesize the desirable SQL queries.

As shown in Figure~\ref{tab:results}, human efforts
spent in providing input-output examples are very limited:
on average, it took less than 5 minutes for one benchmark.
\todo{explain some abnormal points}

The number of interactive rounds is a measure of
the generalization power of the conditional learning part
of the algorithm and the ranking scheme.
We observed that the tool typically requires
just \avground rounds of interaction, when the user is smart
enough to give an example for each input format (which
typically range from 1 to 3) to start with. This was indeed the case
for most cases in our benchmarks, even though our algorithm
can function robustly without this assumption. The maximum number
of interactive rounds required in any scenario was \todo{XXX}
(with 2 to 3 being a more typical number). \todo{the largest table}
The maximum number of examples
required in any scenario over all possible interactions was 10.

\subsubsection{Comparison with an Existing Technique}.
\label{sec:comparison}

We compared \ourtool with \textit{Query By Output} (QBO), a
data-driven approach to infer SQL queries~\cite{Tran:2009}. We chose
QBO because it is the most recent technique and also one
of the most precise SQL query inference techniques in
the literature. QBO requires similar input as \ourtool, and
uses a decision-tree-based algorithm
\todo{explaining what is QBO}
However, QBO cannot infer SQL queries using \todo{aggregates}

The experimental results of QBO is shown in Figure~\ref{tab:results}
(Column ``Query by Example''). For all \exnum database exercises
and \pnum forum questions, QBO produces correct answers
for XXX and XXX of them, respectively. QBO fails to
sythesize desirable SQL queries for other benchmarks, because
it \todo{the reasons}.

We did not compared \ourtool with other related techniques~\cite{},
for three reasons. \todo{reasons}

\subsection{Experimental Discussion}

\noindent \textbf{\textit{Limitations.}}
The experiments indicate three limitations
of our technique. First, the supported SQL
subset may not be expressive enough to write
database queries for certain tasks. \todo{not solved problems} As our
future work, we plan to include more SQL features
to the supported subset, and improve the current
sythesis algorithm.
Second, on some examples, the learned query
condition, though correct, may not be accurate; and requires
users to provide more informative examples.
Take the example input and output in Figure~\ref{fig:rank}
as an example, \ourtool produces a SQL
query \CodeIn{select name from student where score > 3}
\todo{exam}
which perfectly satisify the examples. However, if
the query conditoin of the desirable SQL query
should be \CodeIn{score > 2}, users must provide
one more record in the input table with score 3 
to guide \ourtool to learn the correct query condition.
Third, \ourtool requires
users to provide noise-free input-output examples.
Even in the presence of a small amount of user-input
noises (e.g., a typo), \ourtool will declare failure
when it fails to infer a valid SQL query.
To overcome this limitation, we plan to design a more
robust inference algorithm that can attempt to identify
and tolerate user-input noises, and even suggest a fix
to the noisy example.

\vspace{1mm}
\noindent \textbf{\textit{Threats to Validity.}}
There are three major threats to validity
in our evaluation. First, the \exnum textbook exercises
and \pnum forum questions, though covering
a wide variety of SQL features, may not be representative enough.
Thus, we can not claim the results can be generalized to an
arbitrary scenario. Second, we only compared
\ourtool with the \textit{Query by Output} technique~\cite{Tran:2009}.
Using other query inference or recommendation techniques
might achieve different results. Third, our
experiments focus on evaluating \ourtool's generality 
and accuracy. Even though all experiments are carried
out by a different person other than \ourtool's developers,
it is unknown about \ourtool's general usability for other
end-users. To address this issue, we plan to conduct
a user study in our future work.


\vspace{1mm}
\noindent \textbf{\textit{Experimental Conclusions.}}
We have three chief findings: \textbf{(1)}
The supported SQL subset in \ourtool is
expressive enough to describe a variety of database queries.
\textbf{(2)} \ourtool can efficiently synthesize desirable SQL
queries with a small amount of human
efforts and small input-output examples.
\textbf{(3)} \ourtool produces better results
than an existing technique (\textit{Query by Output}~\cite{Tran:2009}).




