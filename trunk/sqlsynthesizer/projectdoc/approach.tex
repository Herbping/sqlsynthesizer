
\section{Approach}
\label{sec:approach}

Before presenting our SQL query inference algorithm, we first describe
the supported SQL language subset.

\newcommand{\q}{\langle query\rangle}
\newcommand{\db}{\langle db\rangle}
\newcommand{\pat}{\langle pat\rangle}
\newcommand{\bug}{\langle bug\rangle}
\newcommand{\dist}{\langle distance\rangle}
\newcommand{\sem}[1]{\llbracket #1\rrbracket}
\newcommand{\lit}[1]{\texttt{#1}}

\newcommand{\column}{\langle column\rangle}
\newcommand{\dbtable}{\langle table\rangle}
\newcommand{\cond}{\langle cond\rangle}
\newcommand{\op}{\langle op\rangle}
\newcommand{\e}{\langle expr\rangle}
\newcommand{\ce}{\langle cexpr\rangle}

\begin{figure}[t]
%\scriptsize{%
\footnotesize%
\begin{align*}
\q ::= {} 
	& \texttt{ SELECT } \e^+ \texttt{ FROM } \dbtable^+ \\
        & \texttt{ WHERE } \cond^+ \\ 
	&  \texttt{ GROUP BY } \column^+ \texttt{ HAVING } \cond^+\\
\dbtable::= {} &\ atom \\
\column ::= {} &\ \dbtable.atom\\
\cond ::= {} &\ \ \cond \;\texttt{\&\&}\; \cond \\ 
    & |\ \cond \;\texttt{||}\; \cond \\
    & |\ \texttt{(}\;\cond\;\texttt{)} \\
    & |\ \ce \;\op\; \ce \\
\op ::= {} &\ \ \texttt{=} \;\;|\;\; \texttt{>}  \;\;|\;\; \texttt{<}\\
\ce ::= {} &\ \ const \;\;|\;\; \column  \;\; \\
\e ::= {} & \ce \\
    & |\ sum(\column) \;\;|\ max(\column) \;\;|\ min(\column) 
\end{align*}
\normalsize%
\caption{Syntax of the supported SQL language subset.}
\label{fig:syntax}
\end{figure}


\subsection{Language}

Figure~\ref{fig:syntax} defines the syntax of the supported, which
is a subset of the standard query language SQL. This language subset
supports common query operations using multiple tables (i.e.,
\CodeIn{select} ... \CodeIn{from}... \CodeIn{where}..), and standard
boolean operators as predicates. The language
subset shares the same semantics with the standard SQL language.
Different than existing work~\cite{DasSarma:2010}, this language supports
joining operations across multiple tables.  In addition, the language includes
widely used database operations such as \CodeIn{group by},
\CodeIn{order by}, and \CodeIn{having}, as
well as a few common aggregation functions such as \CodeIn{sum},
\CodeIn{max}, and \CodeIn{min}.

Compared to the full SQL specification, this language subset is far from complete.
However, there is a tradeoff between the completeness (or expressiveness) of a language,
and the complexity of finding a simple consistent program (here, a SQL query)
within that space defined by the language. In general, the more expressive
a language, the harder the task of effectively finding an answer. When
designing the language subset, we balanced three inter-related principles:

\begin{itemize}

\item \textit{Expressiveness}. The language subset be able
to represent a reasonable set of SQL queries in practice.

\item \textit{Generality}. The language should resemble
the existing SQL language and should not impose additional
or different semantics.

\item \textit{Tool utility}. Inferring a consistent query
using this language subset should be easy to leverage by
domain knowledges and other analyses.

\end{itemize}

As these principles stand in tension, our supported language subset
represents a sweet spot that addressed each of them to a reasonable
extent. As we will show in Section~\ref{sec:evaluation}, this
language subset is expressive enough for most SQL query exercise
after a widely-used textbook and many online forum answers.


\subsection{Inference Algorithm}

We describe an algorithm to infer SQL queries from 
input-output example. Our algorithm consists of three steps:
table alignment (Section~\ref{sec:alignment}),
query completion (Section~\ref{sec:completion}), and
candidate ranking (Section~\ref{sec:ranking}).

\subsubsection{Table Alignment}
\label{sec:alignment}

In this step, our algorithm scans the given input-output example, aligns the output table data
with the input table data, and infers a query skeleton that captures the basic structure
of a desirable SQL query. The inferred query skeleton is a partial, incomplete SQL query, and
contains possible language structure that can be yet decided.
%, and serves as a good
%reference for further query completion in the next step.

When the algorithm scans the input-output examples, it uses the table \textit{column name}
as the major indicator to establish the input and output table relations. It uses a technique called
\textit{template-based} program synthesis~\cite{sttt-synthesis} to construct a query skeleton as follows:


\begin{itemize}

\item \textit{\textbf{Step 1: Determining Table Set.}} If multiple tables are provided as input, it is very likely that all
provided tables will be used in constructing the desirable SQL query. Based on this observation, we assume that
the minimum number of used tables is the number of all tables provided in the input example. By default, the
table set $T$ used in SQL query contains all given tables. However, it is
possible that a single table will be used in one SQL query for more than once, leading to a more complex query.
Our program does not forbid this case, rather, we \textit{prefer} to use as a few tables as possible to achieve
the same results. In some cases, if one field from a particular input table appears more than once in the
output table, we view it as a strong indicator that this table will be joined multiple times and add it to our table
set $T$ using an alias.


\item \textit{\textbf{Step 2: Determining Joining Columns. }} Given two arbitrary tables, there exist many
ways to join them. Enumerating all possible joining conditions would introduce a huge search space and thus
become intractable. We observe that, in practice, two tables are often joined \textit{only} via the following
three cases: first, tables are joined on their primary keys that have the same (or compatible) data types (i.e., it does not
make any sense to join two tables on a Integer column and a String column). Second, tables are joined
using the columns with the same column name, such as joining a \textit{Student} table with a \textit{Course} table on the
\textit{student\_name} column. Third, two columns that have the data type, and have a large portion of
overlapped data value in the given input can be used as a joining condition. In our tool implementation, it is trivial to check the first 
two cases. For the third case, our algorithm scans the whole given input table to check the ``value similarity''
between two arbitrary columns, and selects columns whose ``value similarity'' is above a fixed threshold as joining candidates.

\item \textit{\textbf{Step 3: Determining Output Columns.}} After determining the table set and joining columns,
the next step is to identify potential column names on which the result would be projected. If a
column in the output table  does not appear in any input table's column list, this output column must
be produced by aggregation. Our algorithm keeps track of these columns and appends a \CodeIn{group by} ... \CodeIn{having} ...
clause to the query skeleton.

\end{itemize}

In summary, this step infers two parts as a query skeleton: a table set used in producing the SQL query, a set of joining conditions
used to connect tables from the table set, and a list of columns used to project the output data.
It is worth noting that the results obtained from the above steps are not \textit{safe} in
terms that they may miss some valid SQL queries. We made the above assumption for the sake of tractability,
since in theory, the bound of table number in a SQL query is $O(n_t!)$, where $n_t$ is the number of given tables;
while the bound of possible number of join is $O(c_t^2)$ and the bound of the number of conditions is $O(n_t!n_tc_t^2)<O(n_t^3c_t^2)$.




%For example, from input-output examples, we can roughly know which
%possible constructions (select, join, group by, or having) will
%be used. This step can reduce the overall search space.

\subsubsection{Query Completion}
\label{sec:completion}

The SQL skeleton produced by the first step, though incomplete, serves as a good reference in inferring complete and valid SQL queries. This step fills in the remaining incomplete parts by heuristic searching.

We use a decision-tree-based \cite{Quinlan:1993} algorithm to figure out the desirable query condition. All tuples which are contained with our output examples are treated like positive examples, while the rest of them are treated as negative ones. All fields of the table are treated like features. In order to deal with the case that two columns in the same table are compared, we add some new binary features (discriminative features) into it. This kind of features should be the answer for questions like \textsf{is T1.C1 greater than T1.C2}. If the answer to that question is true, the value of that feature is $1$ otherwise it's $0$. As a result, our features are the union of original feature and new discriminative features. 

It should be mentioned that usually the output examples are only small part of all candidates tuples, which means positive examples and negative examples are very imbalance. To handle this kind of class imbalance problem, we will use \cite{Chris:2003} instead of simple decision tree.

 
%After each cut, the algorithm tries to ensure all output examples can be kept. To select which column
%is used, the algorithm computes the uncertainty on the rest of columns, and chooses the column with
%the least uncertainty.

After enumerating all possible conditions, the algorithm constructs a valid SQL query and executes it on the input data, and eliminates those producing a different output result.

%How to make a reasonable guess of the remaining parts.

\subsubsection{Candidate Ranking}
\label{sec:ranking}

After the first two steps, several SQL queries satisfying the given input-output examples
will be returned. To reduce the effort in selecting a desirable query, we devise a strategy
to rank the most likely query near the top of the list.
%Due to lacking in input-output examples we are not be able to rule out SQL
%queries that lack in generalization ability. Instead we will provide user queries according to a rank.

Here we use a simple but useful strategy based on Occam's razor to rank all inferred SQL query candidates.
For one output query, each table appearing in it introduces a cost $C_t$ and each appearing condition 
results in an additional cost $C_p$. The total number of cost of a query is $n_t*C_t+n_p*C_p$ in which
$n_t$ is the number of tables and $n_p$ is the number of predicates used in the query. The algorithm
ranks the query based on the cost in an increasing order to ensure that a simpler query often ranks higher.

%If a list of multiple SQL queries satisfy the given input-output examples, how
%to present the most likely queries to the top?


